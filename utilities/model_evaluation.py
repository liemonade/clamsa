import tensorflow as tf
from tensorflow.python.summary.summary_iterator import summary_iterator
from tensorboard.plugins.hparams import plugin_data_pb2
from protobuf_to_dict import protobuf_to_dict
from importlib import import_module
from pathlib import Path
import inspect
from Bio import SeqIO
import numpy as np
import collections
from functools import partial
import itertools

import sys
sys.path.append("..")

from utilities import database_reader
from utilities import msa_converter



# On some versions of CuDNN the default LSTM implementation
# raises a warning. The following code deals with these cases
# See [here](https://github.com/tensorflow/tensorflow/issues/36508)

gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

def get_hyperparameter(path):
    """
        Reads the tf.Event files generated by `hp.hparams` in order to retrieve model hyperparameters
        
        Args:
            path (str): Path to the `events.out.tfevents.*.v2` file
            
        Returns:
            Dict: A dict. with keys given by the names of the hyperparameters and their values
    """

    si = summary_iterator(path)

    for event in si:
        for value in event.summary.value:
            
            proto_bytes = value.metadata.plugin_data.content
            plugin_data = plugin_data_pb2.HParamsPluginData.FromString(proto_bytes)
            
            if plugin_data.HasField("session_start_info"):
                
                hp = plugin_data.session_start_info.hparams
                
                # convert protocol buffer to dict.
                hp = {k: list(protobuf_to_dict(hp[k]).values())[0] for k in hp}
                
                return hp
    return False
            
    


def get_model_information(trial_id, log_dir):
    """
    Find the hyperparameters and model name used in a training run of `utilities.train_models`

    Args:
        trial_id (str): `Trial ID` displayed in the `HParam` panel of Tensorboard of the wanted model
        log_dir (str): Base log directory

    Returns:
        Dict: A dict. of hyperparameters for the wanted model
        str: The name of the model
    """
    
    for path in Path(log_dir).rglob('*.v2'):
        tid = path.parts[-2]
        
        if tid == trial_id:
            
            model_name = path.parts[-3]
            return get_hyperparameter(str(path)), model_name
        
        
        
def get_weights_path(trial_id, saved_weights_dir):
    """
    Finds the weights `*.h5`-file of a trained by model by its Trial ID 

    Args:
        trial_id (str): `Trial ID` displayed in the `HParam` panel of Tensorboard of the wanted model
        saved_weights_dir (str): Base saved weights directory

    Returns:
        str: Path to the `*.h5` file
    """
    
    for path in Path(saved_weights_dir).rglob('*.h5'):
        tid = path.stem
        
        if tid == trial_id:
            
            return str(path)
        
        
def recover_model(trial_id, forest, alphabet_size, log_dir, saved_weights_dir):
    """
    Loads a trained model by its `Trial ID`

    Args:
        trial_id (str): `Trial ID` displayed in the `HParam` panel of Tensorboard of the wanted model
        forest (List[str]): List of path's to Newick files used in the training run
        alphabet_size (int): Number characters in the alphabet used in the training run
        log_dir (str): Base log directory of the training run
        saved_weights_dir (str): Base saved weights directory of the training run

    Returns:
        tf.keras.Model: Trained model
    """
    
    print ("Getting information for model ", trial_id, "in directory", log_dir)
    hps, model_name = get_model_information(trial_id, log_dir)
    print ("The models name is", model_name)
    try:
        model_module = import_module(f"models.{model_name}", package=__name__)
    except ModuleNotFoundError as err:
        raise Exception(f'The module "models/{model_name}.py" for the model "{model_name}" does not exist.') from err
    try:
        create_model = getattr(model_module, "create_model")
    except AttributeError as err:
        raise Exception(f'The model "{model_name}" has no creation function "create_model" in "models/{model_name}.py".')
        
    
    # obtain the types for the default hyperparameter of the create_model function
    signature = inspect.signature(create_model)
    default_hps = {
        k: v.default
        for k, v in signature.parameters.items()
        if v.default is not inspect.Parameter.empty
    }
    default_types = {k: type(default_hps[k]) for k in default_hps}
    
    # convert the loaded hyperparameters to the correct types
    hps = {k: default_types[k](hps[k]) for k in hps}
    
    # obtain the model with the correct weights
    model = create_model(forest, alphabet_size, **hps)
    weights_path = get_weights_path(trial_id, saved_weights_dir)
    model.load_weights(weights_path)
    
    return model







def parse_fasta_file(fasta_path, clades, margin_width=0):
    
    species = [msa_converter.leaf_order(c,use_alternatives=True) for c in clades] if clades != None else []
    
    entries = [rec for rec in SeqIO.parse(fasta_path, "fasta")]
    # parse the species names
    spec_in_file = [e.id.split('|')[0] for e in entries]

    # compare them with the given references
    ref_ids = [[(r,i) for r in range(len(species))  for i in range(len(species[r])) if s in species[r][i] ] for s in spec_in_file]

    # check if these are contained in exactly one reference clade
    n_refs = [len(x) for x in ref_ids]

    if 0 == min(n_refs) or max(n_refs) > 1:
        return None

    ref_ids = [x[0] for x in ref_ids]

    if len(set(r for (r,i) in ref_ids)) > 1:
        return None

    # the first entry of the fasta file has the header informations
    header_fields = entries[0].id.split("|")

    # read the sequences and trim them if wanted
    sequences = [str(rec.seq).lower() for rec in entries]
    sequences = sequences[margin_width:-margin_width] if margin_width > 0 else sequences

    msa = msa_converter.MSA(
        model = None,
        chromosome_id = None, 
        start_index = None,
        end_index = None,
        is_on_plus_strand = True if len(header_fields) < 5 or header_fields[4] != 'revcomp' else False,
        frame = int(header_fields[2][-1]),
        spec_ids = ref_ids,
        offsets = [],
        sequences = sequences
    )
    # Use the correct onehot encded sequences
    coded_sequences = msa.coded_codon_aligned_sequences if use_codons else msa.coded_sequences
    
    # Infer the length of the sequences
    sequence_length = len(coded_sequences[1])  
    
    if sequence_length == 0:
        return None

    # cardinality of the alphabet that has been onehot-encoded
    s = coded_sequences.shape[-1]
    
    # get the id of the used clade and leaves inside this clade
    clade_id = msa.spec_ids[0][0]
    num_species = max([len(specs) for specs in species])
    leaf_ids = [l for (c,l) in msa.spec_ids]
    
    
    # embed the coded sequences into a full MSA for the whole leaf-set of the given clade
    S = np.ones((num_species, sequence_length, s), dtype = np.int32)
    S[leaf_ids,...] = coded_sequences
    
    # make the shape conform with the usual way datasets are structured,
    # namely the columns of the MSA are the examples and should therefore
    # be the first axis
    S = np.transpose(S, (1,0,2))
    
    return clade_id, sequence_length, S

    
    
    
    
def predict_on_fasta_files(trial_ids, # OrderedDict of model ids with keys like 'tcmc_rnn'
                           saved_weights_dir,
                           log_dir,
                           clades,
                           fasta_paths,
                           use_codons = True,
                           batch_size = 30,
                           trans_dict = None,
                           remove_stop_rows = False,
):
    # calculate model properties
    word_len = 3 # codon size or other tuples
    entry_length = word_len if use_codons else 1
    alphabet_size = 4 ** entry_length
    num_leaves = database_reader.num_leaves(clades)
    
    trans_dict = trans_dict if not trans_dict is None else {}
    
    # import the fasta files and filter out empty codon aligned sequences
    path_ids_without_reference_clade = set()
    path_ids_with_empty_sequences = set()
    
    def sequence_generator():

        for f in fasta_paths:

            # filter fasta files that have no valid reference clade
            cid, sl, S = msa_converter.parse_fasta_file(f, clades, trans_dict=trans_dict, remove_stop_rows)
            if cid == -1:
                path_ids_without_reference_clade.add(f)
                continue
            if cid == -2:
                path_ids_with_empty_sequences.add(f)
                continue

            yield cid, sl, S

    
    # load the wanted models and compile them
    models = collections.OrderedDict( (name, recover_model(trial_ids[name], clades, alphabet_size, log_dir, saved_weights_dir)) for name in trial_ids)
    accuracy_metric = 'accuracy'
    auroc_metric = tf.keras.metrics.AUC(num_thresholds = 1000, dtype = tf.float32, name='auroc')
    loss = tf.keras.losses.CategoricalCrossentropy()
    optimizer = tf.keras.optimizers.Adam(0.0005)

    for n in models:
        models[n].compile(optimizer = optimizer,
                          loss = loss,
                          metrics = [accuracy_metric, auroc_metric])


    # construct a `tf.data.Dataset` from the fasta files    
    # generate a dataset for these files
    dataset = tf.data.Dataset.from_generator(sequence_generator, output_types=(tf.int32, tf.int64, tf.float64))

    # batch and reshape sequences to match the input specification of tcmc
    #ds = database_reader.padded_batch(ds, batch_size, num_leaves, alphabet_size)


    padded_shapes = ([], [], [None, max(num_leaves), alphabet_size])
    dataset = dataset.padded_batch(batch_size, 
                                   padded_shapes = padded_shapes, 
                                   padding_values = (
                                       tf.constant(0, tf.int32), 
                                       tf.constant(0, tf.int64), 
                                       tf.constant(1.0, tf.float64)
                                   ))

    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
    dataset = dataset.map(database_reader.concat_sequences, num_parallel_calls = 4)



    # predict on each model
    preds = collections.OrderedDict()

    for n in models:
        model = models[n]
        try:
            preds[n] = model.predict(dataset)[:,1]
        except UnboundLocalError:
            pass # happens in tf 2.3 when there is no valid MSA
        del model

    wellformed_msas = [f for f in fasta_paths if f not in path_ids_with_empty_sequences and f not in path_ids_without_reference_clade]
    preds['path'] = wellformed_msas
    preds.move_to_end('path', last = False) # move MSA file name to front
    
    for p in path_ids_without_reference_clade:
        print(f'The species in "{p}" are not in included in a reference clade. Ignoring it.')
        
    for p in path_ids_with_empty_sequences:
        print(f'The MSA "{p}" is empty (after) codon-aligning it. Ignoring it.')
    
    return preds




def predict_on_tfrecord_files(trial_ids, # OrderedDict of model ids with keys like 'tcmc_rnn'
                           saved_weights_dir,
                           log_dir,
                           clades,
                           tfrecord_paths,
                           use_codons = True,
                           batch_size = 30,
):
    # calculate model properties
    word_len = 3 # codon size or other tuples
    entry_length = word_len if use_codons else 1
    alphabet_size = 4 ** entry_length
    num_leaves = database_reader.num_leaves(clades)
    buffer_size = 1000
    
    
    
    # load the wanted models and compile them
    models = collections.OrderedDict( (name, recover_model(trial_ids[name], clades, alphabet_size, log_dir, saved_weights_dir)) for name in trial_ids)
    accuracy_metric = 'accuracy'
    auroc_metric = tf.keras.metrics.AUC(num_thresholds = 1000, dtype = tf.float32, name='auroc')
    loss = tf.keras.losses.CategoricalCrossentropy()
    optimizer = tf.keras.optimizers.Adam(0.0005)

    for n in models:
        models[n].compile(optimizer = optimizer,
                          loss = loss,
                          metrics = [accuracy_metric, auroc_metric])


    # construct a `tf.data.Dataset` from the fasta files    
    # generate a dataset for these files
    parser = partial(database_reader.parse_tfrecord_entry, num_leaves = num_leaves, alphabet_size=alphabet_size)
    datasets = {p: tf.data.TFRecordDataset(p, compression_type = 'GZIP' if p.endswith('.gz') else None, buffer_size = buffer_size) \
                .map(parser, num_parallel_calls = 2) for p in tfrecord_paths}
    

    # batch and reshape sequences to match the input specification of tcmc
    #ds = database_reader.padded_batch(ds, batch_size, num_leaves, alphabet_size)


    padded_shapes = ([], [], [], [None, max(num_leaves), alphabet_size])
    
    for p in tfrecord_paths:
        
        dataset = datasets[p]
        dataset = dataset.padded_batch(batch_size, 
                                       padded_shapes = padded_shapes, 
                                       padding_values = (
                                           tf.constant(0, tf.int32), 
                                           tf.constant(0, tf.int32), 
                                           tf.constant(0, tf.int64), 
                                           tf.constant(1.0, tf.float64)
                                       ))

        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
        dataset = dataset.map(database_reader.concatenate_dataset_entries, num_parallel_calls = 4)
        datasets[p] = dataset


    # function to extract meta data from the sequences
    from tf_tcmc.tcmc.tensor_utils import BatchedSequences
    bs_layer = BatchedSequences(feature_size = max(num_leaves), dtype=tf.float64, name="batched_sequences")    
    def sequence_data(X, y):
        sequences, clade_ids, sequence_lengths = X
        S = tf.transpose(sequences, perm = [1, 0, 2])
        sl = tf.expand_dims(sequence_lengths, axis=-1)

        nontrivial_entries = tf.logical_not(tf.reduce_all(sequences == tf.ones(64, dtype=tf.float64), axis=-1))
        nontrivial_entries = tf.cast(nontrivial_entries, dtype=tf.float64)
        nontrivial_entries_batched = bs_layer(nontrivial_entries, sl)
        nontrivial_entries_batched = tf.cast(nontrivial_entries_batched, dtype=tf.bool)
        aligned_sequences = tf.reduce_any(nontrivial_entries_batched, axis=1)
        aligned_sequences = tf.reduce_sum(tf.cast(aligned_sequences, dtype=tf.int64), axis=-1)

        model = tf.cast(y[:,1], dtype=tf.int64)
        return (aligned_sequences, sequence_lengths, model)


    # predict on each model
    preds = collections.OrderedDict()
    num_seq = {}
    
    # wanted sequence meta data
    aligned_sequences = None
    sequence_lengths = None
    Y = None

    for p in tfrecord_paths:
        
        dataset = datasets[p]
        
        # evaluate the models
        for n in models:
            model = models[n]
            try:
                pred = model.predict(dataset)[:,1]
                num_seq[p] = pred.shape[0]
                preds[n] = np.concatenate((preds[n], pred)) if n in preds else pred
            except UnboundLocalError:
                pass # happens in tf 2.3 when there is no valid MSA
            del model
            
        # extract the meta data
        for num_ali, sl, y in dataset.map(sequence_data).as_numpy_iterator():
            aligned_sequences = np.concatenate((aligned_sequences, num_ali)) if not aligned_sequences is None else num_ali
            sequence_lengths = np.concatenate((sequence_lengths, sl)) if not sequence_lengths is None else sl
            Y = np.concatenate((Y, y)) if not Y is None else y
                    
    
    filenames = [[p for _ in range(num_seq[p])] for p in tfrecord_paths]
    indices = [list(range(num_seq[p])) for p in tfrecord_paths]
    preds['file'] = list(itertools.chain.from_iterable(filenames))
    preds['index'] = list(itertools.chain.from_iterable(indices))
    preds['aligned_sequences'] = aligned_sequences
    preds['sequence_length'] = sequence_lengths
    preds['y'] = Y

    
    
    preds.move_to_end('aligned_sequences', last = False)
    preds.move_to_end('sequence_length', last = False)
    preds.move_to_end('y', last = False)
    preds.move_to_end('index', last = False)
    preds.move_to_end('file', last = False) 
    
    
    return preds
